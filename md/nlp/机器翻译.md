





[TOC]



以下是CS224N2019 machine translation 课程笔记



### 机器翻译的发展

* 起初的人工翻译
* SMT ( stochastic machine translation)
  * argmax(P(Y|X)) X是指待翻译的句子
  * P(Y|X) = P(X|Y) P(Y) 其中 P(X|Y) 是翻译模型, 而P(Y)是language model
* NMT (neural)



### NMT

![image-20190720162127873](/Users/K/Library/Application Support/typora-user-images/image-20190720162127873.png)

* encoder 编码source sentence
* decoder 第一个时间刻接收来自encoder的编码信息
* 输入start token 开始翻译, 输出 end token 结束翻译



#### beam search

decoder在test阶段，在用beam search 产生句子的时候，停止的标志就是 产生了\<eos\> token

整个beam search 停止的标记就是

* 要么生成到threshold sequence length 大小的句子后 停止
* 要么产生了 threshold 条 sequence 后 停止

如何从生成的句子中挑选出最合适的？

* 直接选score 最高的不合理，因为句子越短 score越大
* ![image-20190720174142120](/Users/K/Library/Application Support/typora-user-images/image-20190720174142120.png)



#### 衡量指标

BLEU (bilingual evaluation understudy)

* n-gram precision
* plus a penalty for too short system translations

* imperfect - cause there are many valid ways to translate a sentence



#### 困境 obstacles

* OOV 
  * sub-word modelling
* domain mismatch
* maintain context over long text
* low resource language pairs



### Hybrid NMT

![image-20190721152802308](/Users/K/Library/Application Support/typora-user-images/image-20190721152802308.png)

在decoder阶段, 生成UNK时，可以把此时的hidden state接到一个char decoder上继续生成词