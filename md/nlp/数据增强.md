

[TOC]



记录几篇关于NLP数据增强的论文阅读笔记



更多关于数据增强的paper 参考[link](https://paperswithcode.com/task/data-augmentation)



#### 同义词替换型

19年ICLR workshop 

[EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks]() 

主要讲了几点在文本分类中用作数据增强的手段

1.Synonym Replacement(SR 同义词替换):  随机取句子中不是停用词的词，替换成对应同义词中的任意一个

2.Random insertion(RI): 随机取句子中不是停用词的词, 从对应同义词中随机抽出一个插在句子的任意位置

3.Random Swap(RS): 随机取两个词，交换位置

4.Random Deletion(RD): 随机删除一个词



##### 参数

1.针对样本集中的一条句子，用这种EDA的方式产生几条句子？

2.以上的四种操作,针对一条句子中的几个词操作？

![image-20190722103708630](/Users/K/Library/Application Support/typora-user-images/image-20190722103708630.png)

其中$\alpha$就是针对句子s(有w个词)，取$\alpha * w$个词进行操作。



##### 可能的问题

文中提到通过上面四种方式做数据增强，可能会存在改变句子分布的情况，也就意味着可能改变句子的label导致增强后的句子的实际label与原始label不匹配，引入错误样本。

![image-20190722104041850](/Users/K/Library/Application Support/typora-user-images/image-20190722104041850.png)

作者做了一个实验显示，EDA前后的样本分布都在一起。 但是保不齐在其他的task上会有改变，所以这可能是在某些task上EDA反而损害model performance的因素。



##### 为什么EDA可以提升分类的performance

1.通过EDA生成的样本引入一定的噪声，防止过拟合

2.EDA的方式也引入了新的词汇，一定程度上缓解测试集中某些词不在训练集中的情况



##### 代码

[代码地址在这个](https://github.com/jasonwei20/eda_nlp)

主要是利用NLTK中的“wordnet”来获取同义词



#### 基于词向量替换

2015 ACL A Lexical and Frame-Semantic
Embedding Based Data Augmentation Approach to Automatic Categorization of Annoying Behaviors using

文中提到了一个用word embedding + KNN 做数据增强，然后在他的任务上F1提高6.1%

> For each word in a tweet, we query the exter- nal embeddings, and replace them with their knn words to create a new training instance. For ex- ample, consider the tweet “Being late is terrible” with the punctuality label, after searching for knn words for each token, we create a new training in- stance: “Be behind are bad” with the same label.



####基于LM的词替换

NAACL 2018  Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations

基于同义词的替换方式是受限的，因为同义词是limited，并且产生的pattern也是受限的。

基于contextual augmentation的方法可以产生更加多样的词，但同时要注意控制生成的词的属性(laebl).

针对样本 The performance are fantastic - positive 这个样本 如果只是找performance的同义词，那么扩充的范围是受限的, 如果通context-LM, 那么像 actors, movies这种都是可以填充上去的。

##### 如何构造这个LM [code](https://github.com/pfnet-research/contextual_augmentation) 用的库是chainer

![image-20190722153042390](/Users/K/Library/Application Support/typora-user-images/image-20190722153042390.png)

先是用LM的方式预训练一个Bi-LSTM. 然后再结合 label来微调模型。

预测替换词的部分，是取预测位置左右的hidden_state 结合label的hidden state



