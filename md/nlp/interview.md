[TOC]





### 模型不收敛有哪些原因

---

https://zhuanlan.zhihu.com/p/36369878

###crf相比hmm等具有哪些优势

---

### lda、pca等降维方法的判断

---



### 维度灾难

---

*  高维度使得聚类变得困难，拥有大量维度意味着彼此相差很大

* 所有样本都靠近样本的边缘

* 随着维度 p的增加，采样密度呈指数下降，因此在没有更多的数据量的情况下，该数据会变得更加稀疏；我们应该进行PCA分析以降低维度

* 高纬度诅咒:随着维度的提升，很多我们在低纬度认为相当然的现象，在高纬度空间里面都不成立了:比如我**们这里提到相邻之间的点全部都变得很远**, 高维空间中余弦相似度未必管用



### 更多的数据就总是更好么？

---

* 取决于你的数据的质量
  * 数据有偏差，获取再多数据也毫无用处

* 取决于你的模型
  * 模型能够承受高偏差，获取更多数据不会太过明显地提高你的测试结果。
  * 你需要添加更多特征，或者做别的处理



### 什么方法让模型对异常值更容忍

---

* L1或L2等正则化方法来减少方差
  * **L2**对大数，对**outlier**更敏感
  * L2 Norm求出来的解是比较均匀的，而L1 Norm常常产生稀疏解

* 换模型
  * 使用基于树的方法来代替回归方法，因为它们更能忍受异常值





#### 各种优化器比较 [参考资料1](https://www.jianshu.com/p/ee39eca29117) [参考资料2](https://cloud.tencent.com/developer/article/1083486)

---

* SGD
* momentum
  * 考虑上一个时间时刻的**梯度**乘以$\alpha$, 
* Nesterov Momentum
  * 该优化器相对于Momentum，唯一不同的是计算反向梯度的时机。Momentum计算的是当前位置的反向梯度，Nesterov Momentum 计算的是按照上次更新方向走一小步后的反向梯度。
* AdaGrad
  * 多了一个**学习率递减**系数
  * 递减系数由**之前所有更新的反向梯度的平方的和**来决定
  * AdaGrad的学习率始终是在减小。
* RMSProp
  * 越早时候计算的梯度对计算衰减系数的影响越小，
  * 这种影响的减小速度就是decay_rate的指数衰减速度

* Adam
  * Adam综合了Momentum的更新方向策略
  * RMProp的计算衰减系数策略



###  MAE(误差绝对值)和MSE的区别

---

* MSE对异常值更加严格。在这个意义上**MAE鲁棒性更好，但也更难以拟合**模型
  * 模型的可变性较小且在计算上**容易拟合**时，我们应该使用MAE，否则用MSE。

* MSE 更容易计算梯度



### bisenet网络的改进目的

---

### BN的作用，为什么能解决梯度爆炸?一般放在哪里？

---

### Dropout的原理

---

### 基于密度的聚类了解哪些

---

### 什么时候选择svm算法，什么时候选择决策树算法。

---

* svm更适合处理特征多的样本。 而决策树处理特征多的样本时容易发生过拟合。