[TOC]





- 分类相比于topic model或者聚类，一个显著的特点是：类目体系是确定的。而不像在聚类和LDA里，一个类被聚出来后，但这个类到底是描述什么的，或者这个类与另外的类是什么关系，这些是不确定的，这样会带来使用和优化上的困难。

* 先用某种无监督的聚类方法，将训练文本划分到某些clusters，建立这些clusters与ODP类目体系的对应关系，然后人工review这些clusters，切分或者合并cluster，提炼name，再然后根据知识体系，建立层级的taxonomy。
* 类目标签数目很多的话，我们一般会将类目标签按照一定的层次关系，建立类目树利用层次分类器来做分类，先对第一层节点训练一个分类器，再对第二层训练n个分类器(n为第一层的节点个数)，一方面单个模型更简单也更准确，另一方面可以避免类目标签之间的交叉影响，但如果上层分类有误差，误差将会向下传导。

* 利用无标记的样本，提出了半监督学习(Semi-supervised Learning)，主要考虑如何利用少量的标注样本和大量的未标注样本进行训练和分类的问题。

  * Self-learning：两个样本集合，Labeled，Unlabeled。执行算法如下

    - 用Labeled样本集合，生成分类策略F
    - 用F分类Unlabeled样本，计算误差
    - 选取Unlabeled中误差小的子集u，加入到Labeled集合。

    接着重复上述步骤。

  * **以前在做page分类器时，先对每一个类人工筛选一些特征词，然后根据这些特征词对亿级文本网页分类，再然后对每一个明确属于该类的网页提取更多的特征词，加入原有的特征词词表，再去做分类；**

    **中间再辅以一定的人工校验，这种方法做下来，效果还是不错的，更关键的是，如果发现那个类有badcase，可以人工根据badcase调整某个特征词的权重，简单粗暴又有效。**

* Co-training：其主要思想是：每次循环，从Labeled数据中训练出两个不同的分类器，然后用这两个分类器对Unlabeled中数据进行分类，把可信度最高的数据加入到Labeled中，继续循环直到U中没有数据或者达到循环最大次数。
* 协同训练，例如Tri-train算法：使用三个分类器.对于一个无标签样本，如果其中两个分类器的判别一致，则将该样本进行标记，并将其纳入另一个分类器的训练样本；如此重复迭代，直至所有训练样本都被标记或者三个分类器不再有变化。
* 半监督学习，**随着训练不断进行，自动标记的示例中的噪音会不断积累，其负作用会越来越大。**所以如term weighting工作里所述，还可以从其他用户反馈环节提取训练数据，类似于推荐中的隐式反馈。



文中用到了Pu-learning去获取高质量的负例样本，具体请参考文献[58]。



* 特征提取
  对于每条instance，运用多种文本分析方法提取特征。常见特征有：
  * 分词 or 字的ngram，对词的权重打分，计算词的一些领域特征，又或者计算词向量，词的topic分布。
    文本串的特征，譬如sentence vector，sentence topic等。
    * 提取的特征，从取值类型看，有二值特征，浮点数特征，离线值特征。
      特征的预处理包括：
      一般来说，我们希望instance各维特征的均值为0，方差为1或者某个有边界的值。如果不是，最好将该维度上的取值做一个变换。
      特征缺失值和异常值的处理也需要额外注意。
      特征选择，下面这些指标都可以用作筛选区分度高的特征。
      Gini-index: 一个特征的Gini-index越大，特征区分度越。
      信息增益(Information Gain)
      互信息(Mutual Information)
      相关系数(Correlation)



- 对于多分类问题，可以选择one-vs-all方法，也可以选择multinomial方法。两种选择各有各的优点，主要考虑有：并行训练multiple class model更复杂；不能重新训练 a subset of topics。