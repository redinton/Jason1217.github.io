





#### 基于词典分词

按照一定的策略将待匹配的字符串和一个**已建立好的“充分大的”词典中的词**进行匹配，若找到某个词条，则说明匹配成功。

* 正向最大匹配法
* 逆向最大匹配法
* 双向匹配分词法

#####最大匹配算法

其寻找最优的排列组合方式是将匹配到的最长词组合在一起，先将词典构造成Trie树

![image-20190613233058857](/Users/K/Library/Application Support/typora-user-images/image-20190613233058857.png)

他说的确实在理  正向匹配 ：他／说／的确／实在／理 

反向最大匹配： 他／说／的／确实／在理 

词典分词虽然可以在O(n)时间对句子进行分词，但是效果很差



##### **最短路径分词算法**

首先将一句话中的所有词匹配出来，构成词图（有向无环图DAG），之后寻找从起始点到终点的**最短路径**作为最佳组合方式

![image-20190613233720637](/Users/K/Library/Application Support/typora-user-images/image-20190613233720637.png)

在求解DAG图的最短路径问题时，总是要利用到一种性质：即两点之间的最短路径也包含了路径上其他顶点间的最短路径。比如S->A->B->E为S到E到最短路径，那S->A->B一定是S到B到最短路径

* **1. 最短路径分词算法**
  * 基于Dijkstra算法求解最短路径。该算法适用于所有带权有向图
    * 但当存在多条距离相同的最短路径时，Dijkstra只保存一条，对其他路径不公平，
* 2.**N-最短路径分词算法**
  * 在每一步保存最短的N条路径，并记录这些路径上当前节点的前驱，在最后求得最优解时回溯得到最短路径。



#####**基于n-gram model的分词**

DAG中的权重不应该为1。

* 常用词的出现频率／概率肯定比罕见词要大。因此可以将求解词图最短路径的问题转化为求解**最大概率路径**的问题。

* 分词结果为“最有可能的词的组合“。

![image-20190613234116347](/Users/K/Library/Application Support/typora-user-images/image-20190613234116347.png)

#### **基于字的分词**

* 基于字的分词事先不对句子进行词的匹配
* 将分词看成序列标注问题，把一个字标记成B(Begin), I(Inside), O(Outside), E(End), S(Single)，也可以看成是每个字的分类问题
* 输入为每个字及其前后字所构成的特征，输出为分类标记



生成式模型和判别式模型

* 两者的本质区别在于X和Y的生成关系。
* 生成式模型以“输出Y按照一定的规律生成输入X”为假设对**P(X,Y)联合概率**进行建模；
  * P(x,y) = P(y)P(x|y)
* 判别式模型认为Y由X决定，直接对后验概率P(Y|X)进行建模。
* 两者各有利弊
  * 生成模型对变量的关系描述更加清晰
  * 而判别式模型容易建立和学习



#### 生成式模型分词

* 生成式模型有 n-gram模型、HMM隐马尔可夫模型、朴素贝叶斯分类等

* 分词中应用较多的是n-gram和HMM
  * 基于字的n-gram就是上面提到的DAG求解最短路径的方式
  * HMM
    * 解决序列标注问题时存在两种序列，
      * 一种是观测序列，即人们显性观察到的句子
      * 一种是隐状态序列, 即序列标签
      * 观测序列为X，隐状态序列是Y，因果关系为Y->X
    * 基于Python的jieba分词器和基于Java的HanLP分词器都使用了HMM。
    * 该模型创建的概率图与上文中的DAG图并不同，因为**节点具有观测**概率
      * 使用Viterbi算法求解最大概率的路径



#### 判别式模型分词算法

判别式模型主要有感知机、SVM支持向量机、CRF条件随机场、最大熵模型等。在分词中常用的有感知机模型和CRF模型。



#####  **平均感知机分词算法**

##### CRF 分词

无向图模型，给定的标注序列Y和观测序列X，对条件概率P(Y|X)进行定义，而不是对联合概率建模。它对未登陆词有很好的识别能力





####基于统计的机器学习

**HMM、CRF、SVM、深度学习**等算法

* stanford、Hanlp分词工具是基于CRF算法

以CRF为例，基本思路是对汉字进行标注训练，不仅考虑了词语出现的频率，还考虑上下文，具备较好的学习能力，因此其对歧义词和未登录词的识别都具有良好的效果。

**机器学习算法和词典相结合，一方面能够提高分词准确率，另一方面能够改善领域适应性。**



#### 分词器当前存在问题

* **分词标准**

  * 比如人名，在哈工大的标准中姓和名是分开的，但在Hanlp中是合在一起的。这需要根据不同的需求制定不同的分词标准。

* **歧义**

  * 对同一个待切分字符串存在多个分词结果
  * 组合型歧义：
    * 分词是有不同的粒度的，指某个词条中的一部分也可以切分为一个独立的词条。比如“中华人民共和国”，粗粒度的分词就是“中华人民共和国”，细粒度的分词可能是“中华/人民/共和国”
  * 交集型歧义
    * 在“郑州天和服装厂”中，“天和”是厂名，是一个专有词，“和服”也是一个词，它们共用了“和”字。

  * 真歧义：
    * 本身的语法和语义都没有问题, 即便采用人工切分也会产生同样的歧义，只有通过上下文的语义环境才能给出正确的切分结果。例如：对于句子“美国会通过对台售武法案”，既可以切分成“美国/会/通过对台售武法案”，又可以切分成“美/国会/通过对台售武法案”。

**一般在搜索引擎中**，构建索引时和查询时会使用不同的分词算法。**常用的方案是，在索引的时候使用细粒度的分词以保证召回，在查询的时候使用粗粒度的分词以保证精度。**



#### 是否有必要分词

* word-level 表示的不足
  * **词数据稀疏问题**不可避免地导致过拟合，而且大量的OOV限制了模型的学习能力
  * 分词方法不统一与分词效果欠佳



* 参考论文 *Is Word Segmentation Necessary for Deep Learning of Chinese Representations* 
  * 部分问题上 char-level 表现优于 word-level



####如何选择分词工具

根据业务场景有选择地应用上述算法，

* 比如在搜索引擎对**大规模网页**进行内容解析时，对**分词对速度要求大于精**度，
* 而在智能问答中由于句子较短，对分词的**精度要求大于速**度。



####专业领域分词

* 仅通过添加专业词汇 解决一些特定专业的问题

  * 社交网络场景 - 网络用语 目前仅能通过人工添加词汇
    * 新词挖掘算法表现不佳
  * 时间，单位场景
    * “2019年1月28日”切分成“2019/年/1/月/28日 不合理 日期都应该是一个整体
    * 正则表达式进行替换
    * 时间、人名、地名要用NER来辅助分词

* 方案

  * 规则的查词典方案

    * 不断用新词发现算法维护一个动态更新的词表

  * 深度学习

    * 标注集缺失的问题

    

####**中文分词数据集有哪些？**

1. 滨州大学中文树库（Chinese Penn Treebank, CTB）
2. 北京大学云计算语言所 标注的人民日报语料
3. SIGHAN的国际中文分词评测数据集 SIGHAN Bakeoff-2003等
4. 新浪微博语料



分词算法根据其核心思想主要分为两种，

* 第一种是基于字典的分词，先把句子按照字典切分成词，再寻找词的最佳组合方式；
* 第二种是基于字的分词，即由字构词，先把句子分成一个个字，再将字组合成词，寻找最优的切分策略，同时也可以转化成序列标注问题。